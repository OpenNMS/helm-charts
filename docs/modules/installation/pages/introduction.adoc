= Introduction
:imagesdir: ../assets/images
:!sectids:

OpenNMS Helm Charts is based and tested against the latest Horizon.
See the following for version compatibilty:

[options="header"]
[cols="1,1,1"]
|===

| Helm chart version
| Horizon version
| Meridian version

| 1.x
| Horizon 32.x
| Meridian 2023.x
|===

[[requirements]]
== Requirements

Before you begin, ensure that you have a deep understanding of how Kubernetes and Helm work.

The following describes the requirements for your local machine, Kubernetes, and additional external dependencies.
We do not yet support cloud deployments.

=== Local

You must have the following installed on your machine:

* https://kubernetes.io/docs/reference/kubectl/[kubectl]
* https://helm.sh/docs/intro/install/[Helm] version 3
* (optional) https://minikube.sigs.k8s.io/docs/start/[minikube] for testing

//Do we need to include minimum OS requirements?

When using cloud resources, make sure you have https://learn.microsoft.com/en-us/cli/azure/reference-index?view=azure-cli-latest[az] for Azure or https://cloud.google.com/sdk/gcloud[gcloud] for Google Cloud.

=== Kubernetes

* Kubernetes version 1.20+.
* A single `namespace`, which represents a single OpenNMS environment or customer deployment or a single tenant.
It must not contain special characters and must follow FQDN restrictions.
* A single instance of OpenNMS Core (backend) for centralized monitoring running ALEC in standalone mode (if enabled).
** OpenNMS does not support distributed mode, meaning the `StatefulSet` cannot have more than one replica.
* A shared volume for the RRD files, mounted as read-write on the Core instance.
* A shared volume for the core configuration files.
** Allows for sharing configuration across all the OpenNMS instances (for example, `users.xml`, and `groups.xml`).
* Multiple instances of Grafana (frontend), using PostgreSQL as the backend, pointing to the OpenNMS UI service when available.
** The OpenNMS Helm Chart data sources point to the OpenNMS Core service.
* `Secrets` to store the credentials, certificates, and truststores.
* `ConfigMaps` to store initialization scripts and standard configuration settings.
* An `Ingress` to control TLS termination and provide access to all the components (using Nginx).footnote:[You can manage certificates using Let's Encrypt via `cert-manager`, but we only require the name of a `ClusterIssuer`.]

For more information about Ingress, see the xref:reference:ingress.adoc[reference section].

*Optional requirements for additional implementation scenarios*

* Multiple instances of read-only OpenNMS UI (frontend).
** Must be stateless (unconfigurable).
** The `Deployment` must work with multiple replicas.
** Any configuration change goes to the core server.

* Multiple instances of Sentinel to handle Flows (requires Elasticsearch as an external dependency).
** When Sentinels are present, `Telemetryd` is disabled on the OpenNMS Core instance.

IMPORTANT: Containers are bundled with default plugins.
You can add additional plugins by building your own container image (see https://github.com/OpenNMS/helm-charts/blob/main/kar-container/README.md[kar-container] for an example).

=== External dependencies

Kafka, Elasticsearch, and PostgreSQL running externally (and maintained separately from the solution), all with SSL enabled.

* PostgreSQL server as the central database for OpenNMS and Grafana.
** For Google Cloud, the solution was tested using Google SQL for PostgreSQL with SSL and a private IP.

* Elasticsearch cluster for flow persistence.

* Grafana Loki server for log aggregation.
** https://grafana.com/docs/loki/latest/getting-started/logcli/[logcli] helps extract OpenNMS logs from the command line for troubleshooting purposes.

* https://cert-manager.readthedocs.io/en/latest/[cert-manager] to provide HTTPS/TLS support to the web-based services the ingress controller manages.
** A `ClusterIssuer` to use it across multiple independent OpenNMS installations.

* Nginx Ingress Controller, as the solution has not been tested with other Ingress implementations.
